![MIT LICENSE](https://shields.io/badge/license-MIT-green)
![python 3.8](https://img.shields.io/badge/python-3.8-brightgreen)
# ðŸ”¥ Disentangling Writer and Character Styles for Handwriting Generation

 <p align='center'>
  <b>
    <a href="https://arxiv.org/abs/2303.14736">ArXiv</a>
    |
    <a href="https://github.com/dailenson/SDT/blob/master/static/Poster_SDT.pdf">Poster</a>
    | 
    <a href="https://youtu.be/mKbYLEwa4dI">Video</a>
    | 
    <a href="https://cvpr2023.thecvf.com/virtual/2023/poster/20954">Project</a>
  </b>
</p> 

## ðŸ“¢ Introduction
- The proposed style-disentangled Transformer (SDT) generates online handwritings with conditional content and style. 
- Existing RNN-based methods mainly focus on capturing a personâ€™s overall writing style, neglecting subtle style inconsistencies between characters written by the same person. In light of this, SDT disentangles the writer-wise and character-wise style representations from individual handwriting samples for enhancing imitation performance. 
- We extend SDT and introduce an offline-to-offline framework for improving the generation quality of offline Chinese handwritings.

# ðŸ”¥ SDT with connected dataset


## ðŸ”¨ Requirements
```
conda create -n sdt python=3.8 -y
conda activate sdt
# install all dependencies
conda env create -f environment.yml
```

## ðŸ“‚ Folder Structure
  ```
  SDT/
  â”‚
  â”œâ”€â”€ train.py - main script to start training
  â”œâ”€â”€ test.py - generate characters via trained model
  â”œâ”€â”€ evaluate.py - evaluation of generated samples
  â”‚
  â”œâ”€â”€ configs/*.yml - holds configuration for training
  â”œâ”€â”€ parse_config.py - class to handle config file
  â”‚
  â”œâ”€â”€ data_loader/ - anything about data loading goes here
  â”‚   â””â”€â”€ loader.py
  â”‚
  â”œâ”€â”€ model_zoo/ - pre-trained content encoder model
  â”‚
  â”œâ”€â”€ data/ - default directory for storing experimental datasets
  â”‚
  â”œâ”€â”€ model/ - networks, models and losses
  â”‚   â”œâ”€â”€ encoder.py
  â”‚   â”œâ”€â”€ gmm.py
  â”‚   â”œâ”€â”€ loss.py
  â”‚   â”œâ”€â”€ model.py
  â”‚   â””â”€â”€ transformer.py
  â”‚
  â”œâ”€â”€ saved/
  â”‚   â”œâ”€â”€ models/ - trained models are saved here
  â”‚   â”œâ”€â”€ tborad/ - tensorboard visualization
  â”‚   â””â”€â”€ samples/ - visualization samples in the training process
  â”‚
  â”œâ”€â”€ trainer/ - trainers
  â”‚   â””â”€â”€ trainer.py
  â”‚  
  â””â”€â”€ utils/ - small utility functions
      â”œâ”€â”€ util.py
      â””â”€â”€ logger.py - set log dir for tensorboard and logging output
  ```



